{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.dataset_loader import DatasetLoader\n",
    "from configs.datasets.dataset_config import DATASETS, CATEGORIES\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf715b",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DatasetLoader()\n",
    "\n",
    "# Load AdvBench\n",
    "print(\"Loading AdvBench...\")\n",
    "advbench = loader.load_advbench()\n",
    "print(f\"âœ“ Loaded {len(advbench)} examples\\n\")\n",
    "\n",
    "# Show example\n",
    "print(\"Example:\")\n",
    "print(advbench[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffe3f7",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b19ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(advbench)\n",
    "\n",
    "# Prompt length distribution\n",
    "df['prompt_length'] = df['prompt'].str.split().str.len()\n",
    "\n",
    "print(\"Prompt Length Statistics:\")\n",
    "print(df['prompt_length'].describe())\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df['prompt_length'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Prompt Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prompt Lengths in AdvBench')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433e2d7",
   "metadata": {},
   "source": [
    "## 3. Sample Prompts by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb899d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show random samples\n",
    "print(\"Random Adversarial Prompts:\\n\")\n",
    "for i, example in enumerate(df.sample(5).itertuples(), 1):\n",
    "    print(f\"{i}. {example.prompt}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5120d4",
   "metadata": {},
   "source": [
    "## 4. Compare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13280670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple datasets\n",
    "datasets_info = []\n",
    "\n",
    "for name, config in DATASETS.items():\n",
    "    try:\n",
    "        dataset = loader.load_dataset(name)\n",
    "        datasets_info.append({\n",
    "            'name': config['name'],\n",
    "            'size': len(dataset),\n",
    "            'license': config['license']\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {name}: {e}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(datasets_info)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03323aee",
   "metadata": {},
   "source": [
    "## 5. Prepare Preference Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab96d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of creating preference pairs\n",
    "from src.data.prepare_datasets import DPODatasetPreparator\n",
    "\n",
    "preparator = DPODatasetPreparator()\n",
    "\n",
    "# Create safety preferences for sample prompts\n",
    "sample_prompts = df['prompt'].head(5).tolist()\n",
    "preferences = preparator.create_safety_preferences(sample_prompts)\n",
    "\n",
    "print(\"Example Preference Pair:\\n\")\n",
    "print(f\"Prompt: {preferences[0]['prompt']}\")\n",
    "print(f\"\\nChosen (Safe): {preferences[0]['chosen']}\")\n",
    "print(f\"\\nRejected (Unsafe): {preferences[0]['rejected']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799ce39",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Prepare full datasets: `python src/data/prepare_datasets.py --dataset all`\n",
    "2. Explore training configurations in `configs/training/`\n",
    "3. See model selection in `configs/models/model_config.py`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
